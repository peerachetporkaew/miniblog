---
title: "ทำความรู้จักกับระบบแปลภาษาอัตโนมัติ Transformer"
mathjax: true
comments : true
layout: post
published: true
categories: NLP
---

## เกริ่นนำ

รู้มั้ยว่า Transformer นอกจากจะเป็นภาพยนต์แนวหุ่นยนต์จากนอกโลกสุดล้ำแล้ว ชื่อนี้ยังเป็นชื่อของระบบแปลภาษาอัตโนมัติ (Machine Translation) ที่ทางกูเกิลได้คิดค้นขึ้นมาอีกด้วย บทความนี้จะมาเกริ่นนำคร่าวๆ (คร่าวๆ มากๆๆ) ว่าระบบแปลภาษาอัตโนมัติแบบ Transformer ทำงานอย่างไร



เทคโนโลยีระบบแปลภาษาอัตโนมัติในปัจจุบันก้าวหน้าไปมาก ถึงแม้คอมพิวเตอร์สามารถเข้าใจภาษามนุษย์ได้ในวงจำกัด แต่ด้วยปริมาณข้อมูลมหาศาลในยุคปัจจุบัน ทำให้นักวิจัยสามารถเก็บข้อมูลคำศัพท์ และตัวอย่างประโยคจากอินเตอร์เน็ตมาฝึกสอนให้กับคอมพิวเตอร์ ผ่านทางระบบปัญญาประดิษฐ์แบบโครงข่ายประสาทเทียมได้ ซึ่งอดีตโครงข่ายประสาทเทียม หรือ Artificial Neural Network นั้น ประกอบด้วยจำนวนเซลประสาท (neuron) เพียงไม่กี่สิบ หรือไม่กี่ร้อยเซลเท่านั้น ความสามารถอย่างเก่งก็เพียงแค่สามารถรู้จำ และแยกแยะตัวเลขหรือตัวหนังสือออกได้

แต่ด้วยความสามารถของคอมพิวเตอร์ในปัจจุบัน ทำให้เราสามารถสร้างผังการคำนวณ (Computation Graph) ของโครงข่ายประสาทเทียมที่ประกอบด้วยเซลประสาทได้สูงถึงหลักสิบล้านหรือร้อยล้านเซลได้ ความสำเร็จเบื้องหลังนี้ต้องขอบคุณเหล่าเกมเมอร์ทั้งหลายที่ทำให้การ์ดจอราคาถูกลง  เพราะการคำนวณโครงข่ายประสาทเทียมเหล่านี้อาศัยหลักการของระบบเมทริกซ์ (Matrix) ซึ่งใช้ในคำนวณการแสดงผลภาพ 3D บนการ์ดจอสำหรับเกมยุคปัจจุบันที่ต้องการความสมจริงและคมชัด 

ด้วยความสามารถทางการคำนวณและปริมาณข้อมูลที่เพิ่มขึ้นทำให้คอมพิวเตอร์สามารถจดจำและทำนายสิ่งต่างๆ จากสัญญาณที่เข้ามาได้โดยตรงโดยไม่จำเป็นต้องอาศัยกระบวนการสกัดคุณลักษณะ (feature extraction) เหมือนแต่ก่อน กล่าวคือ สมัยก่อนหากจะแยกแยะประเภทดอกไม้ เราต้องป้อนความยาวกลีบ สี จำนวนกลีบ ให้คอมพิวเตอร์ไปสร้างแบบจำลองคณิตศาสตร์เพื่อคำนวณประเภทออกมา แต่ปัจจุบันเราสามารถป้อนภาพเข้าไปตรงๆ โดยไม่ต้องวัดความยาวเหล่านี้เลย เพราะตัวปัญญาประดิษฐ์จะไปเรียนรู้และสกัดเอาคุณลักษณะที่สำคัญออกมาได้โดยอัตโนมัติ 

## แล้ว Transformer แปลภาษาได้อย่างไร

เทคโนโลยีแปลภาษาอัตโนมัติเกิดขึ้นมาหลายสิบปีแล้ว ในยุคแรกๆ อาศัยมนุษย์เป็นผู้เขียนกฏการแปล ป้อนพจนานุกรม คำศัพท์ สำนวนต่างๆ เข้าไป ซึ่งสามารถแปลได้แต่ประโยคง่ายๆ เท่านั้น ต่อมาทฤษฏีทางสถิติได้รับการพัฒนาขึ้น ประกอบกับข้อมูลที่มีก็มากขึ้น ทำให้คอมพิวเตอร์สามารถเรียนรู้คำแปลได้เองจากการอ่านคู่ประโยคจำนวนมาก วิธีการแปลแบบสถิติก็ยังมีข้อจำกัดอยู่ที่ เวลาจะแปลส่วนใดส่วนหนึ่งจำเป็นต้องไปหาส่วนที่เหมือนกันในฐานข้อมูล (หรือเรียกว่า ตารางคู่คำแปล) ขึ้นมา ดังนั้นหากเจอประโยคที่พลิกแพลงไปก็จะทำให้ระบบไม่อาจแปลในส่วนนั้นได้ ในยุคของการแปลเชิงสถิตินี้มีคำศัพท์ใหม่ของระบบแปล เรียกว่า Decoder หรือตัวถอดรหัส ที่สมมติว่าเราจะแปลจากภาษาอังกฤษเป็นไทยนั้น เรามีสมมติฐานอยู่ว่า ภาษาอังกฤษคือภาษาไทยที่ถูกเข้ารหัสไว้ หากเราเรียนรู้สถิติของการเข้ารหัส เราก็สามารถที่จะถอดรหัสได้นั่นเอง ซึ่งความเป็นจริงแล้วการแปลมีความสลับซับซ้อนกว่าการถอดรหัสปกติอยู่มาก ทำให้ระบบนี้มีข้อจำกัดพอสมควร

ปัจจุบัน วิธีการที่นักวิจัยทางภาษาใช้นั้นจึงใช้หลักการของ Distributed Representation กล่าวคือ แทนที่จะประมวลจากคำ แบบเป็นสายอักขระ ก็เปลี่ยนคำ ให้กลายเป็นเวกเตอร์ของจำนวนจริงแทน ขนาดของเวกเตอร์อาจจะแตกต่างกันไปในแต่ระบบ ส่วนมากอาจมีจำนวนถึง 500 หรือ 1000 ก็ได้ 

ในยุคบุกเบิกของระบบแปลภาษาแบบโครงข่ายประสาทเทียมนั้น นักวิจัยหลายท่านได้ใช้ หลักการของ Recurrent Neural Network เพื่ออ่านเวกเตอร์ของประโยคภาษาต้นทาง และแปลงให้กลายเป็น เวกเตอร์ ที่เป็นตัวแทนของประโยคต้นทางนั้น จากนั้นใช้ Recurrent Neural Network อีกชุดหนึ่งเพื่อสร้างคำแปลขึ้นมา วิธีการนี้เรียกว่า encoder-decoder framework

ต่อมามีผู้พัฒนากลไล attention ขึ้น ซึ่งกลไกนี้ ทำให้ระหว่างที่ decoder กำลังสร้างคำแปลนั้น สามารถค้นหาส่วนของประโยคในภาษาต้นทาง เหมือนกับเลือกส่วนของประโยคมาแปลโดยอัตโนมัติได้ด้วย วิธีการนี้ได้รับความนิยมสูงมาก และยังใช้อยู่ในปัจจุบัน

ระบบแปลภาษาของ Transformer ก็ประกอบด้วย encoder-decoder เช่นเดียวกัน เพียงแต่กระบวนการแปลงประโยคนั้น จะไม่ค่อยๆ อ่านทีละคำ แต่จะแปลงไปพร้อมๆ กันทั้งประโยค โดยอาศัยหลักการของ Self-attention กล่าวคือ เวกเตอร์ของทุกๆ คำจะถูกผสมผสานกันตามน้ำหนัก หรือ attention ที่คำนวณได้ การผสมผสานกันนี้จะทำวนไปหลายๆ รอบ จนได้เป็นเมทริกซ์ ที่เป็นตัวแทนของประโยค

เมทริกซ์นี้ จะถูกนำไปใช้ใน Decoder ซึ่งอาศัยหลักการของ Self-attetion และ Decoder-Encoder Attention ในทำนองเดียวกันเพื่อสร้างคำแปลขึ้นมา ผลลัพธ์จากตัว Decoder ก็คือ ลำดับของเวกเตอร์ที่มีขนาดเท่ากับจำนวนศัพท์ในภาษาปลายทาง ลำดับนี้มีความยาวเท่ากับความยาวประโยคผลลัพธ์ เราก็จะสามารถเลือกคำศัพท์ตามลำดับที่ให้ค่าสูงสุดในเวกเตอร์ตำแหน่งนั้นๆ มาเป็นประโยคคำตอบได้ หรืออาจจะใช้วิธีการค้นหาคำตอบที่ดีที่สุดแบบ Beam Search ก็ได้เช่นกัน

![title](assets/transformer/model_arch.png)

โดยหลักการของการทำงานโดยสังเขปก็มีประมาณนี้ ซึ่งยังมีส่วนประกอบอื่นๆ ที่ยังไม่ได้กล่าวถึงในบทความนี้อีก อาทิเช่น การฝึกสอนโครงข่ายประสาทเทียมทำได้อย่างไร องค์ประกอบย่อยๆ ใน Transformer มีอะไรทำงานอย่างไรบ้าง ซึ่งก็คงต้องให้น้องๆ ศึกษาเพิ่มเติมตามความชอบเอานนะครับ ^_^" ถ้าลงลึกไปมากกว่านี้รับรองว่าคนอ่านหลับแน่ๆ ครับ

### สรุป

ไปดู Transformer ดีกว่า